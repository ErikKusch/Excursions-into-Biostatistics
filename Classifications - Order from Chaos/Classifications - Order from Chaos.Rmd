---
title: "Classifications"
subtitle: "Order from Chaos"
author: "Erik Kusch"
date: "18/11/2020"
fontsize: 10pt
output: 
  beamer_presentation: 
    keep_tex: true # keep latex file that is generated during compilation
    toc: false # this is added through a later command
    slide_level: 3 # at how many pound signs (#) to assume slide title level
    includes:
      in_header: Style.tex
classoption: c # change to `handout` to ignore breaks
---
  
```{r setup, include=FALSE}
options(scipen=1, digits=4)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, cache.lazy=TRUE, tidy.opts=list(width.cutoff=50),tidy=FALSE, fig.height=8, digits=4)
```

<!--- ####### TOC (use this if you want ## headers to be included in the toc)--------------------
---------------------------------------  --->
  
\tableofcontents

<!--- ####### DOCUMENT --------------------
---------------------------------------  --->

# Variables
### Types of Variables
Variables can be classed into a multitude of types. The most common classification system knows:
\vspace{0.6cm}
\begin{columns}[T]
  \begin{column}{.5\linewidth}
  \pause
    \textbf{Categorical Variables}
    \begin{itemize}
    \item also known as \textit{Qualitative Variables}
    \item Scales can be either:
      \begin{itemize}
      \item Nominal
      \item Ordinal
      \end{itemize}
    \end{itemize}
  \end{column}
  \begin{column}{.5\linewidth}
  \pause
    \textbf{Continuous Variables}
    \begin{itemize}
    \item also known as \textit{Quantitative Variables}
    \item Scales can be either:
      \begin{itemize}
      \item Discrete
      \item Continuous
      \end{itemize}
    \end{itemize}
  \end{column}
\end{columns}

## Categorical Variables
### Categorical Variables
\begin{tcolorbox}[colback=bondiblue!5,colframe=bondiblue!40!black,title=]
\centering Categorical variables are those variables which \textbf{establish and fall into distinct groups and classes}.
\end{tcolorbox}
\pause
\vspace{0.4cm}
Categorical variables:
\begin{itemize}
\item can take on a finite number of values
\item assign each unit of the population to one of a finite number of groups
\item can \textit{sometimes} be ordered
\end{itemize}
\pause
\vspace{0.4cm}
\textbf{In R}, categorical variables usually come up as object type `factor` or `character`.

### Categorical Variables (Examples)
Examples of categorical variables:
\vspace{.5cm}
\begin{itemize}
\pause
\item Biome Classifications (e.g. "Boreal Forest", "Tundra", etc.)
\item Sex (e.g. "Male", "Female")
\item Hierarchy Position (e.g. "$\alpha$-Individual", "$\beta$-Individual", etc.)
\item Soil Type (e.g. "Sandy", "Mud", "Permafrost", etc.)
\item Leaf Type (e.g. "Compound", "Single Blade", etc.)
\item Sexual Reproductive Stage (e.g. "Juvenile", "Mature", etc.)
\item Species Membership
\item Family Group Membership
\item ...
\end{itemize}

## Continuous Variables
### Continuous Variables
\begin{tcolorbox}[colback=bondiblue!5,colframe=bondiblue!40!black,title=]
\centering Continuous variables are those variables which \textbf{establish a range of possible data values}.
\end{tcolorbox}
\pause
\vspace{0.4cm}
Continuous variables:
\begin{itemize}
\item can take on an infinite number of values
\item can take on a new value for each unit in the set-up
\item can \textit{always} be ordered
\end{itemize}
\pause
\vspace{0.4cm}
\textbf{In R}, continuous variables usually come up as object type `numeric`.

### Continuous Variables (Examples)
Examples of categorical variables:
\vspace{.5cm}
\begin{itemize}
\pause
\item Temperature
\item Precipitation
\item Weight
\item pH
\item Altitude
\item Group Size
\item Vegetation Indices
\item Time
\item ...
\end{itemize}

## Converting Variable Types
### Binning Variables
\textit{Continuous variables} can be converted into \textit{categorical variables} via a method called \textbf{binning:}

\vspace{0.1cm}

\pause
Given a variable range, one can establish however many "bins" as one wants. For example:
\pause
\begin{itemize}
\item Given a temperature range of $271K - 291K$, there may be 4 bins of equal size:
  \begin{itemize}
  \item Bin A: $271K \leq X \leq 276K$
  \item Bin B: $276K < X \leq 281K$
  \item Bin C: $281K < X \leq 286K$
  \item Bin D: $286K < X \leq 291K$
  \end{itemize}

\end{itemize}
\vspace{0.2cm}
\pause
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering Whilst a \textbf{continuous variable} can be both \textit{continuous} and \textit{categorical}, a \textbf{categorical variable} can only ever be \textit{categorical}!
\end{tcolorbox}

### Confusion Of Units
\begin{center}
\includegraphics[width=0.5\linewidth]{Figures/Metrics.jpg}  
\end{center}

# Classifications
## Logistic Regression
### Theory
\textbf{Logistic Regression} \flushright `glm(..., family=binomial(link='logit'))` in base `R`  
\vspace{-5pt}
\begin{table}[c]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{L{0.2\linewidth}L{0.8\linewidth}}
    \textit{Purpose:} &  Understand how certain variables drive distinct outcomes.\\
    \pause
    \textit{Assumptions:} & 
      \begin{itemize}
      \item Down to \textit{Study-Design}:
      \begin{itemize}
      \item Variable values are \textbf{independent} (not paired)
      \item \textit{Binary logistic regression}: response variable is \textbf{binary}
      \item \textit{Ordinal logistic regression}: response variable is \textbf{ordinal}
      \end{itemize}
      \item Need for \textit{Post-Hoc Tests}:
      \begin{itemize}
      \item Absence of \textbf{influential outliers}
      \item Absence of \textbf{multi-collinearity}
      \item Predictor Variables and \textbf{log odds} are related in a \textbf{linear fashion}
      \end{itemize}
      \end{itemize}
     \\
  \end{tabular}}
\end{table}


### Example - The Data
\footnotesize
```{r Logistic1}
library(titanic)
train_df <- na.omit(titanic_train) # remove NA rows
test_df <- train_df[c(1:50),c(2,3,5,6)] # 50 data for testing
train_df <- train_df[c(-1:-50),c(2,3,5,6)] # remaining data for training
head(train_df)
```

\vspace{.5cm}
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering Can we explain \textbf{Survival} (`Survived`) based on \textit{Passenger class} (`Pclass`), \textit{sex} (`Sex`), and \textit{age} (`Age`). Was it really "Women and children first"?
\end{tcolorbox}

### Example - The Model
\footnotesize
```{r Logistic2}
Logistic_Mod <- glm(Survived ~., # use all variables in the data frame
                    family = binomial(link = 'logit'), # logistic
                    data = train_df # where to take the data from
                    )
summary(Logistic_Mod)[["coefficients"]]
```

\vspace{.5cm}
\begin{tcolorbox}[colback=bondiblue!5,colframe=bondiblue!40!black,title=]
\centering Logistic Regression Coefficients can't be interpreted the same way as regular linear model coefficients since we are interested in survival probabilities between $0$ and $1$.
\end{tcolorbox}

### Example - Explanation \& Prediction
\footnotesize
\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=]
\centering Clearly, women of a young age in first class had the highest survival rate.
\end{tcolorbox}

How do we know this? As *class* increases (from 1 to 3), survival probability decreases (`r summary(Logistic_Mod)[["coefficients"]][2,1]`). Furthermore, men (*sexmale*) had, on average, a much lower survival rate than women (`r summary(Logistic_Mod)[["coefficients"]][3,1]`). Lastly, increasing *age* negatively affected survival chances (`r summary(Logistic_Mod)[["coefficients"]][4,1]`).  

\vspace{.05cm}

But how sure can we be of our model accuracy? We can test it by **predicting** some new data and **validating** our predictions:
```{r Logistic3}
# predict on test data
fitted <- predict(Logistic_Mod, newdata=test_df, type='response') 
# if predicted survival probability above .5 assume survival
fitted <- ifelse(fitted > 0.5 , 1, 0) 
# compare actual data with predictions --> ERROR RATE
mean(fitted != test_df$Survived) 
```

\vspace{.05cm}
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering In reality, one would fine-tune the probability at which to assume survivorship!
\end{tcolorbox}

## K-Means
### Theory
\textbf{K-Means Clustering} \flushright `Mclust()` in `mclust` package  
\vspace{-5pt}
\begin{table}[c]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{L{0.2\linewidth}L{0.8\linewidth}}
    \textit{Purpose:} &  Identify a number of $k$ clusters in our data.\\
    \pause
    \textit{Assumptions:} & 
      \begin{itemize}
      \item Variance of the distribution of each variable is spherical
      \item All variables have the same variance
      \item Prior probability for all $k$ clusters are the same
      \end{itemize}
     \\
  \end{tabular}}
\end{table}

\vspace{.5cm}
\begin{tcolorbox}[colback=bondiblue!5,colframe=bondiblue!40!black,title=]
\centering `mclust` is capable of identifying the statistically most appropriate number of clusters for the data set.
\end{tcolorbox}

### Example - The Data I
\footnotesize
```{r K1}
data("iris")
head(iris)
```

\vspace{.5cm}
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering Can we accurately identify the `Species` contained within the data set by clustering according to `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`?
\end{tcolorbox}

Here, we decide to limit the number of clusters to the number of species present so we can test how well the prediction went.

### Example - The Data II

\footnotesize

\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering When building a \textit{training} and \textit{test} data set for identification of discrete values, we need to identify data of each group in both data sets. We do so via \textbf{stratified sampling}.
\end{tcolorbox}

```{r}
library(splitstackshape) # access to the stratified function
set.seed(42) # make sampling reproducible
test_df <- stratified(indt = iris, # input data
           group = "Species", # what the strata are
           size = 7, # how many samples per strata
           keep.rownames = TRUE) # keep the original rownames
training_df <- iris[-as.numeric(test_df$rn), ] # training data
```

\vspace{.5cm}
\begin{tcolorbox}[colback=bondiblue!5,colframe=bondiblue!40!black,title=]
\centering Doing this assures that we have data for each group to build a classifier as well as test the validity of our grouping.
\end{tcolorbox}

### Example - The Model I
\footnotesize
```{r K2, fig.height = 4.5}
library(mclust)
Mclust_mod <- Mclust(training_df[,-5], # data for the cluster model
                   G = length(unique(training_df[,5]))) # group number
plot(Mclust_mod, what = "uncertainty")
```

### Example - The Model II
\footnotesize
\begin{tcolorbox}[colback=bondiblue!5,colframe=bondiblue!40!black,title=]
\centering Looking at the cluster centres and/or spreads can help with some \textbf{biological interpretation}.
\end{tcolorbox}
```{r}
Mclust_means <- Mclust_mod[["parameters"]][["mean"]] # extract means
colnames(Mclust_means) <- unique(training_df$Species) # set columns
Mclust_means
```
\vspace{.5cm}

I prefer a visualization as seen on the previous slide. 

### Example - Explanation \& Prediction
\footnotesize
Clearly, `Petal.Length`, and `Petal.Width` are extremely good separators for our different clusters with the green and red clusters (`versicolor` and `virginica`) overlapping a lot in `Sepal.Length` and `Sepal.Width` space.  

\vspace{.3cm}

But how sure can we be of our model accuracy? We can test it by **predicting** the cluster membership and **validating** our predictions against the real data:
```{r K3}
Mclust_pred <- predict.Mclust(Mclust_mod, iris[,-5]) # prediction
fitted <- Mclust_pred$classification # predicted species number
# compare actual data with predictions --> ERROR RATE
mean(fitted != as.numeric(iris$Species))
```

<!-- ## Support-Vector Machines -->
<!-- ### Theory -->

<!-- ### Example - The Data -->
<!-- ```{r SVM1} -->

<!-- ``` -->

<!-- ### Example - The Model I -->
<!-- ```{r SVM2} -->

<!-- ``` -->

<!-- ### Example - The Model II -->
<!-- ```{r SVM3} -->

<!-- ``` -->

## Hierarchies
### Theory
\textbf{Hierarchical Clustering} \flushright `hclust()` in base `R` or `rpart()` in `rpart` package and many others
\vspace{-5pt}
\begin{table}[c]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{L{0.25\linewidth}L{0.75\linewidth}}
    \textit{Purpose:} &  Build a decision tree for classification of our data.\\
    \pause
    \textit{Advantages:} & 
      \begin{itemize}
      \item Very easy to \textbf{explain and interpret}.
      \item Easy to \textbf{visualize}.
      \item Easily handle qualitative predictors without the need to create dummy variables.
      \vspace{-20pt}
      \end{itemize} \\
     \textit{Disadvantages:} & 
      \begin{itemize}
      \item Very \textbf{sensitive to} the \textbf{choice of linkage}.
      \item Generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.
      \item Trees can be very \textbf{non-robust}.
      \end{itemize}
     \\
  \end{tabular}}
\end{table}

### Example - The Data I
\footnotesize
```{r H1a}
data("iris")
head(iris)
```
\vspace{.5cm}
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering Again, let's see if we can accurately identify the `Species` contained within the data set by clustering according to `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`.
\end{tcolorbox}

### Example - The Data II \& Model I
\footnotesize
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering `hclust()` can only handle distance matrices.
\end{tcolorbox}
\vspace{.2cm}
We a distance matrix between the numeric components of our data like so:
```{r H1b}
dist_mat <- dist(iris[, -5])
```
\vspace{.2cm}
\begin{tcolorbox}[colback=bondiblue!5,colframe=bondiblue!40!black,title=]
\centering A distance matrix stores information about the dissimilarity of different observations.
\end{tcolorbox}

\vspace{.5cm}
\pause
Now, we can build our initial model:
```{r}
clusters <- hclust(dist_mat, method = "complete")
```

### Example - The Model II
\footnotesize
```{r H2, fig.height = 5}
par(mfrow = c(1,3))
plot(clusters, main = "complete")
plot(hclust(dist_mat, method = "single"), main = "single")
plot(hclust(dist_mat, method = "average"), main = "average")
```

### Example - Explanation \& Prediction
\footnotesize
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering Hierarchical clustering recognises as many groups as there are observation and we may wish to \textbf{prune} the decision tree to a meaningful split level.
\end{tcolorbox}
\vspace{.2cm}

We know that we have three species in our data, so we may want to cut the complete tree at a height of $3$ (not because that's the number of species, but because the tree just so happens to recognize three clusters at that level of decision-making).

```{r H3}
clusterCut <- cutree(clusters, 3) # cut tree
table(clusterCut, iris$Species) # assess fit
```

As we can see here, our decision tree has had no issue identifying *setosa* and *versicolor* into clusters $1$ and $2$ respectively. However, it is struggling with placing the species *virginica*.

### Example - Decisions
\footnotesize
So far we weren't able to tell the actual decision rules of how to cluster our data. Let's do this:  
\footnotesize
```{r H4, fig.height = 4.1}
library(rpart)
fit <- rpart(Species ~. , data = iris)
plot(fit, margin = .1); text(fit, use.n = TRUE)
```

\vspace{-.5cm}

\pause
\raggedleft We can tell that our decisions for assigning species membership build on `Petal.Length` and `Petal.Width` in this example (remember the K-mean clustering)!

## Random Forests
### Theory
\textbf{Random Forests} \flushright `tuneRF()` in `randomForest` package
\vspace{-5pt}
\begin{table}[c]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{L{0.25\linewidth}L{0.75\linewidth}}
    \textit{Purpose:} &  Identify which variables to use for clustering our data and build a tree.\\
    \pause
    \textit{Advantages:} & 
      \begin{itemize}
      \item Extremely \textbf{powerful}.
      \item Very \textbf{robust}.
      \item Easy to \textbf{interpret}.
      \vspace{-20pt}
      \end{itemize} \\
     \textit{Disadvantages:} & 
      \begin{itemize}
      \item A \textbf{black box} algorithm.
      \item \textbf{Computationally expensive}.
      \end{itemize}
     \\
  \end{tabular}}
\end{table}

### Example - The Data
\footnotesize
```{r RF1}
data("iris")
head(iris)
```
\vspace{.5cm}
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering One final time, we ask whether we can accurately identify the `Species` contained within the data set by clustering according to `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`.
\end{tcolorbox}

### Example - The Model
\footnotesize
```{r RF2}
library(randomForest)
set.seed(42) # set seed because the process is random
RF_Mod <- tuneRF(x = iris[,-5], # variables which to use for clustering
                 y = iris[,5], # correct cluster assignment
                 strata = iris[,5], # stratified sampling
                 doBest = TRUE, # run the best overall tree
                 ntreeTry = 20000, # consider this number of trees
                 improve = 0.0001, # improvement if this is exceeded
                 trace = FALSE, plot = FALSE)
RF_Mod[["confusion"]]
```


### Example - Explanation
\footnotesize
\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=]
\centering That is one \textbf{stunningly accurate} classification!
\end{tcolorbox}

\vspace{.3cm}
Let's see which variables where actually the most useful when making our clustering decisions:  
```{r RF3, fig.height = 4}
varImpPlot(RF_Mod)
```

<!-- \pause -->
<!-- Again, `Petal.Width` and `Petal.Length` alone seem to be almost enough to accurately classify all of our `iris` data in their respective species memberships! -->

## Networks
### Theory
\textbf{Network Clustering} \flushright `cluster_optimal()`, etc. in `igraph` package and many others
\vspace{-5pt}
\begin{table}[c]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{L{0.25\linewidth}L{0.75\linewidth}}
    \textit{Purpose:} &  Identify compartments which are strongly connected within, but not between each other.\\
    \pause
    \textit{Advantages:} & 
      \begin{itemize}
      \item Highly \textbf{flexible} approaches.
      \item Network analyses \textbf{offer much more} than clustering.
      \item Allow for clustering of \textbf{very different data} and identification relationships than other approaches.
      \vspace{-20pt}
      \end{itemize} \\
     \textit{Disadvantages:} & 
      \begin{itemize}
      \item \textbf{Steep learning curve}.
      \item Tricky in \textbf{formatting data correctly}.
      \item Choices can become \textbf{overwhelming}
      \end{itemize}
     \\
  \end{tabular}}
\end{table}

### Example - The Data
\footnotesize

Here, we take a foodweb contained within the `foodwebs` data collection of the `igraphdata` package. We are using the Middle Chesapeake Bay in Summer foodweb (\tiny Hagy, J.D. (2002) Eutrophication, hypoxia and trophic transfer efficiency in Chesa-peake Bay PhD Dissertation, University of Maryland at College Park (USA), 446 pp. \footnotesize).
\vspace{.3cm}
```{r N1}
library(igraph)
library(igraphdata)
data("foodwebs")
Foodweb_ig <- foodwebs[[2]]
```
\vspace{.5cm}
\begin{tcolorbox}[colback=burgundy!5,colframe=burgundy!40!black,title=]
\centering Let's see what kind of network-internal clusters we can make out.
\end{tcolorbox}

### Example - A Directed Network
\tiny
\vspace{-.5cm}
\begincols[T]
  \begincol{.4\linewidth}
  \footnotesize
  \vspace{1cm}
  A **directed network** is one in which we know which node/vertex is acting one which other node/vertex.  
  
  \vspace{.3cm}
  
  We identify the clusters as follows:
  \tiny
```{r N2, fig.width=7, eval = FALSE}
Clusters <- cluster_optimal(Foodweb_ig)
Colours <- Clusters$membership
Colours <- rainbow(max(Colours))[Colours]
plot(Foodweb_ig, 
     vertex.color = Colours,
     vertex.size = degree(Foodweb_ig)*0.5, 
     layout=layout.grid, edge.arrow.size=0.001)
```
  \footnotesize
  This identifies sub-networks/clusters by optimizing the modularity score of the overall network (i.e. optimizing connections within vs. between clusters).
  \endcol
  \begincol{.6\linewidth}
```{r N2b, fig.width = 7, echo = FALSE}
Clusters <- cluster_optimal(Foodweb_ig)
Colours <- Clusters$membership
Colours <- rainbow(max(Colours))[Colours]
plot(Foodweb_ig, 
     vertex.color = Colours,
     vertex.size = degree(Foodweb_ig)*0.5, 
     layout=layout.grid, edge.arrow.size=0.001)
```
  \endcol
\endcols

### Example - An Undirected Network
\tiny
\vspace{-.5cm}
\begincols[T]
  \begincol{.4\linewidth}
  \footnotesize
  \vspace{1cm}
  An **undirected network** is one in which we don't know which node/vertex is acting one which other node/vertex.  
  
  \vspace{.3cm}
  
  We identify the clusters as follows (there are more options):  
  \tiny
```{r N3, fig.width=7, eval = FALSE}
Foodweb_ig <- as.undirected(Foodweb_ig)
Clusters <- cluster_fast_greedy(Foodweb_ig)
Colours <- Clusters$membership
Colours <- rainbow(max(Colours))[Colours]
plot(Foodweb_ig,
     vertex.color = Colours,
     vertex.size = degree(Foodweb_ig)*0.5,
     layout=layout.grid, edge.arrow.size=0.001)
```
  \footnotesize
  This identifies sub-networks/clusters by optimizing the modularity score of the overall network (i.e. optimizing connections within vs. between clusters).
  \endcol
  \begincol{.6\linewidth}
```{r N3b, fig.width=7, echo = FALSE}
Foodweb_ig <- as.undirected(Foodweb_ig)
Clusters <- cluster_fast_greedy(Foodweb_ig)
Colours <- Clusters$membership
Colours <- rainbow(max(Colours))[Colours]
plot(Foodweb_ig,
     vertex.color = Colours,
     vertex.size = degree(Foodweb_ig)*0.5,
     layout=layout.grid, edge.arrow.size=0.001)
```
  \endcol
\endcols